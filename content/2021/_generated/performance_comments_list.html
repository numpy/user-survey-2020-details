
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>&lt;no title&gt; &#8212; NumPy Community Survey 2020  documentation</title>
    
    <link href="../../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
  
    
    <link rel="stylesheet"
      href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
    <link rel="preload" as="font" type="font/woff2" crossorigin
      href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
    <link rel="preload" as="font" type="font/woff2" crossorigin
      href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">
  
    
      
  
    
    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/pydata-sphinx-theme.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.css" />
    
    <link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">
  
    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="../../../index.html">
  <img src="../../../_static/numpylogo.svg" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../demographics.html">
  Community
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../contributions.html">
  Contributions
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../priorities.html">
  Priorities
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../features_and_deprecations.html">
  Usage
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../biggest_impact.html">
  Future
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../acknowledgements.html">
  Acknowledgements
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../index.html">
  Previous Years
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../glossary.html">
  Glossary
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar">
              <div class="sidebar-start-items"><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search survey results ..." aria-label="Search survey results ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    
  </div>
</nav>
              </div>
              <div class="sidebar-end-items">
              </div>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="simple visible nav section-nav flex-column">
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Comments</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Allowing Numpy to access to GPU to parallel tasks</p></td>
</tr>
<tr class="row-odd"><td><p>It‚Äôs already pretty good, I don‚Äôt know how it can get better</p></td>
</tr>
<tr class="row-even"><td><p>I mean this as ‚Äúperformance should be a top priority‚Äù. numpy already performs well- I think this should continue to be a priority.</p></td>
</tr>
<tr class="row-odd"><td><p>Specifically, performance on windows, though I‚Äôm not sure if this is feasible.</p></td>
</tr>
<tr class="row-even"><td><p>The performance is mostly great. The survey forced me to select some priority order and hence I think performance, reliability and documentation are for my work the most important ones. In my experience numpy is fast, reliable and has clear documentation.</p></td>
</tr>
<tr class="row-odd"><td><p>Solicit algorithm optimization from the public. (The comment was made in Mandarin.)</p></td>
</tr>
<tr class="row-even"><td><p>Paralelization. Tutorial on best practices. Advanced tutorials on numexpr and numba.</p></td>
</tr>
<tr class="row-odd"><td><p>Enabling the multi-threading via env variables for a ML stack consisting of NumPy, SciPy, SciKit-Learn and other higher-level libraries is very tricky in a production codebase (setting the env variable before NumPy is first imported etc.) and currently doesn‚Äôt work for our codebase at all (while it works on a trivial PoC code snippet). Detection and exclusion of hyper-threaded cores is missing. Dynamic vs. static thread count setting has undocumented but significant impact. All this matters in on-premise deployments a lot.</p></td>
</tr>
<tr class="row-even"><td><p>Nothing to complain, but I highly value numpys performance</p></td>
</tr>
<tr class="row-odd"><td><p>There‚Äôs still a lot of Numpy code that could probably be easily multi-threaded, which would give a huge performance improvement on a lot of my code. I know that there are dedicated libraries out there but that significantly complicates installation and maintenance (in particular within companies with brain-dead download policies), therefore I try to keep imports to a minimum.</p></td>
</tr>
<tr class="row-even"><td><p>multicore support for basic operations would be nice</p></td>
</tr>
<tr class="row-odd"><td><p>CUDA support. Support for AMD‚Äôs BLAS</p></td>
</tr>
<tr class="row-even"><td><p>Using GPU and parallelisation</p></td>
</tr>
<tr class="row-odd"><td><p>In terms of speed Numpy is amazing, maybe I am more concerned in terms of RAM consumption, specifically for a numpy masked array</p></td>
</tr>
<tr class="row-even"><td><p>Vector operation related. (The comment was made in Japanese.)</p></td>
</tr>
<tr class="row-odd"><td><p>Continue honing the low-level (C) code.</p></td>
</tr>
<tr class="row-even"><td><p>Join forces with projects like JAX and Dask. Develop better interoperability with modern Fortran through f2py or LFortran.</p></td>
</tr>
<tr class="row-odd"><td><p>Developing NumPy for developers from underdeveloped countries. Developers from economically underdeveloped counties have less high powered PC configuration, thus creating NumPy to run on lower PC config. with high performance will be helpful for them.</p></td>
</tr>
<tr class="row-even"><td><p>I think documentation on how to write faster numpy code would be helpful: vectorizing, etc. Add the ability to use units with calculations.</p></td>
</tr>
<tr class="row-odd"><td><p>Performance on Windows is worse than on Linux on the same computer</p></td>
</tr>
<tr class="row-even"><td><p>Using SIMD code to leverage hardware vectorization.</p></td>
</tr>
<tr class="row-odd"><td><p>better facilities for using numpy with GPU‚Äôs/CUDA</p></td>
</tr>
<tr class="row-even"><td><p>Reduce overhead</p></td>
</tr>
<tr class="row-odd"><td><p>make it faster</p></td>
</tr>
<tr class="row-even"><td><p>Provide a min/max method to compute min/max in one go rather than from min and then max.</p></td>
</tr>
<tr class="row-odd"><td><p>How much processing can be handled in small computers and big data without implying the need of costly GPUs, memory/processors</p></td>
</tr>
<tr class="row-even"><td><p>Developing smaller modules that allow a faster response in calculations. (The comment was made in Spanish.)</p></td>
</tr>
<tr class="row-odd"><td><p>Allow for multiprocessing without being more verbose.</p></td>
</tr>
<tr class="row-even"><td><p>Numpy could use extended instruction sets or target GPU.</p></td>
</tr>
<tr class="row-odd"><td><p>When applying arbitrary Python code with broadcast. (The comment was made in Japanese.)</p></td>
</tr>
<tr class="row-even"><td><p>Performance is good now but one of the main reasons I use <code class="docutils literal notranslate"><span class="pre">numpy</span></code> is performance. I don‚Äôt have strong feelings about e.g., new features, website updates, etc.</p></td>
</tr>
<tr class="row-odd"><td><p>Multi node MKL</p></td>
</tr>
<tr class="row-even"><td><p>Ya guys r doing fine‚Ä¶ just don‚Äôt change the syntax‚Ä¶. let it last as pristine as possible</p></td>
</tr>
<tr class="row-odd"><td><p>Spreading more in the Spanish language, sorry if I‚Äôm selfish! (The comment was made in Spanish.)</p></td>
</tr>
<tr class="row-even"><td><p>CuPy being native so that large arrays, which run slow for me, which is probably a CPU issue, not a numpy issue. Though added GPU support would be fantastic.</p></td>
</tr>
<tr class="row-odd"><td><p>Make improvement in calculation speed. Some people from my field move to Julia because of that</p></td>
</tr>
<tr class="row-even"><td><p>I‚Äôd love to have integrated profiling tools, giving hits to bottlenecks in matrix operations and suggesting alternatives. And a closer integration with Numba would be very nice too!</p></td>
</tr>
<tr class="row-odd"><td><p>SIMD instructions for modern processors (as you have been doing), more ambitiously, processing backends such as OpenCL and Vulkan.</p></td>
</tr>
<tr class="row-even"><td><p>Numpy is the fundamental toolbox on almost thousands of software development chaintool.  If some functionality can be improved to run faster, it will have tremendous implications in a vast array of applications.</p></td>
</tr>
<tr class="row-odd"><td><p>None in particular, but any performance gain can have huge impacts.</p></td>
</tr>
<tr class="row-even"><td><p>There are functions such as np.unique that are incompatible with numba, so I want a function with narrowed functions. (The comment was made in Japanese.)</p></td>
</tr>
<tr class="row-odd"><td><p>Improve call overhead</p></td>
</tr>
<tr class="row-even"><td><p>Enhanced vectorization, multithreaded/SIMD map feature</p></td>
</tr>
<tr class="row-odd"><td><p>- More use of SIMD operations - Maybe some integration with Numba for automatic ‚Äúfusion‚Äù of operations? Not sure about this one‚Ä¶</p></td>
</tr>
<tr class="row-even"><td><p>Automated performance tests. Perhaps some way of improving performance for the vectorize function if that is at all possible.</p></td>
</tr>
<tr class="row-odd"><td><p>Profile memory and report CPU and memory usage with VTune.</p></td>
</tr>
<tr class="row-even"><td><p>Improve GPU support.</p></td>
</tr>
<tr class="row-odd"><td><p>Would love anything I can get to make my code faster ;)</p></td>
</tr>
<tr class="row-even"><td><p>Honestly, numpy is amazingly performant. Could the overhead of talking with python be cut down somehow?</p></td>
</tr>
<tr class="row-odd"><td><p>I don‚Äôt think there are any easy answers.  Supporting and pushing projects that make use of DAG and JIT is probably the best way forward.  The challenge here is tht this is really outside of core NumPy.</p></td>
</tr>
<tr class="row-even"><td><p>Better integration with CuPy (GPU), multithreaded computation, accessible options for tailoring your personal build (CPU architecture, BLAS library‚Ä¶.) ideally with one simple and easy to use script to rebuild the package. Support for float16</p></td>
</tr>
<tr class="row-odd"><td><p>Native optimizations for the Apple M1 chip</p></td>
</tr>
<tr class="row-even"><td><p>Actually more docs on performance would be cool</p></td>
</tr>
<tr class="row-odd"><td><p>Extra compiled routines for a few key operations (like 2D regular binned histograms, AKA images). Mostly this is just something to keep in a priority list, since som many people depend on NumPy for compute-intensive codes.</p></td>
</tr>
<tr class="row-even"><td><p>Operations on masked arrays can be pretty slow.</p></td>
</tr>
<tr class="row-odd"><td><p>I guess I don‚Äôt think NumPy underperforms, I just would like to see it do an even more awesome job. My models involve a lot of applying finite difference methods to PDEs and running them 10^6 times so any bit of performance helps!</p></td>
</tr>
<tr class="row-even"><td><p>Vectorising all the APIs, universal function support.</p></td>
</tr>
<tr class="row-odd"><td><p>This is tough. I cannot think of anything off the top of my head, since NumPy is rather well designed in most aspects.</p></td>
</tr>
<tr class="row-even"><td><p>Magically make my existing numpy code faster üôÇ</p></td>
</tr>
<tr class="row-odd"><td><p>Further weight reduction and speeding up of processing. (The comment was made in Japanese.)</p></td>
</tr>
<tr class="row-even"><td><p>It‚Äôs already pretty good, but its main weak point is that it produces intermediate arrays‚Äîany tricks that can lead to more operator-fusion is appreciated!</p></td>
</tr>
<tr class="row-odd"><td><p>better support for AMD processors</p></td>
</tr>
<tr class="row-even"><td><p>something like what julia is currently doing with llvm and simd</p></td>
</tr>
<tr class="row-odd"><td><p>Further JAX integration. Also with Scipy</p></td>
</tr>
<tr class="row-even"><td><p>Some operations could take advantage of fusing of operators and this improve memory bandwidth of the code</p></td>
</tr>
<tr class="row-odd"><td><p>It can be improved if there are multiple people working at the maintainance of NumPy from wide range of knowledge and interests.</p></td>
</tr>
<tr class="row-even"><td><p>- Optional TensorFlowesque chaining of operations to save memory bandwidth - Bespoke low-level code optimized to major architectures - Ability to offload code to accelerators (e.g. GPU), especially if this can be done in a way that‚Äôs transparent to the user</p></td>
</tr>
<tr class="row-odd"><td><p>Device placement</p></td>
</tr>
<tr class="row-even"><td><p>Numpy is already very high performance, but performance matters a lot to me.  I use numpy because the apis are great and I rarely have to think about performance.</p></td>
</tr>
<tr class="row-odd"><td><p>Most of my APIs import it, and therefore any improvements to imports will have a broad impact on my projects‚Äô import times</p></td>
</tr>
<tr class="row-even"><td><p>I would like an easy user story with GPU and multi-core setups.</p></td>
</tr>
<tr class="row-odd"><td><p>User-extendable dtypes</p></td>
</tr>
</tbody>
</table>


              </div>
              
              
              <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
              
          </main>
          

      </div>
    </div>
  
    <script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>
  <footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2020, NumPy Survey Team.<br>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.4.0.<br>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>