
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>&lt;no title&gt; &#8212; NumPy Community Survey 2020  documentation</title>
    
    <link href="../../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
  
    
    <link rel="stylesheet"
      href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
    <link rel="preload" as="font" type="font/woff2" crossorigin
      href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
    <link rel="preload" as="font" type="font/woff2" crossorigin
      href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">
  
    
      
  
    
    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/pydata-sphinx-theme.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.css" />
    
    <link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">
  
    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="../../../index.html">
  <img src="../../../_static/numpylogo.svg" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../2021/demographics.html">
  Community
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../2021/contributions.html">
  Contributions
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../2021/priorities.html">
  Priorities
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../2021/features_and_deprecations.html">
  Usage
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../2021/biggest_impact.html">
  Future
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../2021/acknowledgements.html">
  Acknowledgements
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../index.html">
  Previous Years
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../glossary.html">
  Glossary
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar">
              <div class="sidebar-start-items"><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search survey results ..." aria-label="Search survey results ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    
  </div>
</nav>
              </div>
              <div class="sidebar-end-items">
              </div>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="simple visible nav section-nav flex-column">
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Comments</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>I dont know enough what goes under the hood in NumPy, but still at least for me it is the top priority</p></td>
</tr>
<tr class="row-odd"><td><p>As NumPy is a library that is widely used both professionally and academically, it is certain to guarantee high performance and reliability. (Original in Portuguese: Como NumPy é uma biblioteca de vasto uso tanto em âmbito profissional como acadêmico, é certo garantir alta performance e confiabilidade.)</p></td>
</tr>
<tr class="row-even"><td><p>Since NumPy operations are at the core of so many programs, any performance improvement will have a significant impact on many applications. NumPy is by no means slow, but optimizing its performance would be nice.</p></td>
</tr>
<tr class="row-odd"><td><p>Give <code class="docutils literal notranslate"><span class="pre">numpy.random.Generator.choice</span></code> an output parameter</p></td>
</tr>
<tr class="row-even"><td><p>- slow on small arrays - better integration with pypy - why not a jit / or integration of numba - more transparent access to GPU (even though it’s quite good already)</p></td>
</tr>
<tr class="row-odd"><td><p>GPU support</p></td>
</tr>
<tr class="row-even"><td><p>Parallel support with numba, vectorization of functions and speed improvements, possible implementation with something like arrow</p></td>
</tr>
<tr class="row-odd"><td><p>many functionalities can be sped up using numba, it would be good if this gap could be closed</p></td>
</tr>
<tr class="row-even"><td><p>by improving user-base interaction by continuous updates of features by emphasizing on key learning objectives  by working on tutorials</p></td>
</tr>
<tr class="row-odd"><td><p>Proper and dynamic support for vector extensions across the entire library, i.e. current versions of AVX etc. numpy should auto-detect the capabilities of CPUs and choose the fastest option for every operation.</p></td>
</tr>
<tr class="row-even"><td><p>Many projects are addressing performance issues around data manipulation. As a core component, maintaining performance is important.</p></td>
</tr>
<tr class="row-odd"><td><p>More parallelization and GPU support</p></td>
</tr>
<tr class="row-even"><td><p>Mostly I think Numpy is very performant - the changes to the fft backends are greatly appreciated. I just think that maintaining this performance would be good - making use of multi-threaded operations would be good, but I know this is outside of scope for NumPy generally and I use other libraries with a NumPy-like API when I need this.</p></td>
</tr>
<tr class="row-odd"><td><p>Some operations take too long time.</p></td>
</tr>
<tr class="row-even"><td><p>No particular place - I just think that as the existing np code is already quite well documented, etc. Performance is the most important aspect of a scientific/numerical library.</p></td>
</tr>
<tr class="row-odd"><td><p>Open to this improving however possible.</p></td>
</tr>
<tr class="row-even"><td><p>Facilitate the use of multiple threads or processes for operations that require high computing power. (Original in Spanish: Facilitar el uso de múltiples hilos o procesos para las operaciones que requieran alto poder de computo.)</p></td>
</tr>
<tr class="row-odd"><td><p>It should always be a priority. It is for me what makes numpy great. It is fast!</p></td>
</tr>
<tr class="row-even"><td><p>I want the ability to use the GPU.</p></td>
</tr>
<tr class="row-odd"><td><p>As manually added, I think GPU-processing mit help. I‘m in the field of image algorithm development so mostly work on image data (image sequences as I‘m working on digital cinema technology) Performance isn‘t that bad btw. just the thing that can‘t be improved to much.</p></td>
</tr>
<tr class="row-even"><td><p>I actually select Others &amp; Performance. As I’m using NumPy extensively for image processing, being able to directly address the GPU without requiring CuPy, for example, is extremely important if not critical.</p></td>
</tr>
<tr class="row-odd"><td><p>I believe that the numba project is very promising, and I would really love to see better cooperation between numba and numpy.</p></td>
</tr>
<tr class="row-even"><td><p>Make it easier to use numba, dask, cython</p></td>
</tr>
<tr class="row-odd"><td><p>You guys are doing a great job, more connection to c++ And other high performance languages by promoting and explaining functionality in the documentation and media outlets, understanding and learning deeper abstractions by using numpy</p></td>
</tr>
<tr class="row-even"><td><p>NumPy to be competitive performance with compiler-based tools that encourage less elegant programming styles</p></td>
</tr>
<tr class="row-odd"><td><p>parallel processing, utilizing specialized hardware (optionally)</p></td>
</tr>
<tr class="row-even"><td><p>I get complaints about numpy vs matlab perfomance, mostly from people who’ve never used numpy.  I know numpy is about as fast as Matlab, but haters gonna hate and taters gonna tate.</p></td>
</tr>
<tr class="row-odd"><td><p>Improve computational efficiency, especially for large arrays. (Original in Japanese: 提高计算效率，尤其针对大数组.)</p></td>
</tr>
<tr class="row-even"><td><p>I code optimization solvers that have to do many small matrix-vector multiplications as part of the function, as in they can not be folded into some higher matrix operation, etc.  I realize that this is a rather niche use case but is there anyway to bring down the calling overhead? For example a Ax with being a 5x5 is effectively no different in cost to A being a 30x30 on my machine. I think a small matrix optimization would do wonders.  Also default linking to something other then MKL</p></td>
</tr>
<tr class="row-odd"><td><p>Perhaps an integrated integration with numba, so when you call a function on an array you could specify a how=numba type parameter</p></td>
</tr>
<tr class="row-even"><td><p>Potentially include the use of GPUs.</p></td>
</tr>
<tr class="row-odd"><td><p>Specify arrays of fixed size so that certain functions run faster. I work a lot with small matrices, mostly for linear algebra stuff, so it would be nice to have functions optimized for certain sizes of arrays.</p></td>
</tr>
<tr class="row-even"><td><p>Nothing in particular. I think as one of the go-to libraries for scientific computing Numpy should strive to maintain and improve the performance of its underlying functions, for current and future features.</p></td>
</tr>
<tr class="row-odd"><td><p>Could NumPy run on GPUs?</p></td>
</tr>
<tr class="row-even"><td><p>Numpy is key for the Machine Learning.. Need Performance is the key factor</p></td>
</tr>
<tr class="row-odd"><td><p>more use of parallel computing</p></td>
</tr>
<tr class="row-even"><td><p>Ability to automatically hand off certain operations to Intel MKL, Blitz, … libraries when available on the system and faster.</p></td>
</tr>
<tr class="row-odd"><td><p>I don’t have any ideas on how to improve it but it seems to me that it is always the most important thing and why it is used. I would love to collaborate on whatever development priority is taken. (Original in Spanish: No tengo ideas de como mejorar pero me parece que siempre es lo más importante y por lo que se lo usa. Me encantaría colaborar en cualquier prioridad que se tome de desarrollo.)</p></td>
</tr>
<tr class="row-even"><td><p>I know NumPy already has a lot of vectorization and paralelization, but maybe including automatic paralelization using GPUs or coprocessors. Probably similar to what JAX does but built in in some core components of NumPy.</p></td>
</tr>
<tr class="row-odd"><td><p>By testing, indexing with numpy took much longer than the same function but with numba’s njit decorator. Perhaps you could adopt certain improvements that are invisible to the user. (Original in Spanish: Haciendo pruebas, el realizar un indexing con numpy se demoró mucho más que la misma función pero con el decorador njit de numba. Tal vez se podría adoptar ciertas mejoras que sean invisibles al usuario.)</p></td>
</tr>
<tr class="row-even"><td><p>I just wanted to express that the most important thing for me would be the improved performance whenever possible. Probably incorporating parallelism in linear algebra libraries would be a good idea not that complicated to execute. (Original in Spanish: Lamentablemente no se me ocurre como solo quise expresar que lo más importante para mi sería que mejoren cada vez que pueden el rendimiento. Probablemente incorporar paralelismo en las librerías de álgebra lineal sería una buena idea no tan complicada de ejecutar. )</p></td>
</tr>
<tr class="row-odd"><td><p>Use a lighter implementation closest to the function calls on cython. But without changes on the code, something easy to enable and disable. Using decorators maybe?</p></td>
</tr>
<tr class="row-even"><td><p>Default inter-core parallelization options. (Original in Spanish: Opciones de paralelizacion entre núcleos por defecto.)</p></td>
</tr>
<tr class="row-odd"><td><p>I don’t have any suggestions, I just think that performance should be prioritized.</p></td>
</tr>
<tr class="row-even"><td><p>More explanation on how to write more performant code</p></td>
</tr>
<tr class="row-odd"><td><p>I think numpy performance is spectacular, but I believe it should continue to be a focus.</p></td>
</tr>
<tr class="row-even"><td><p>I have few good thoughts about this, I just believe that performance is most important.</p></td>
</tr>
<tr class="row-odd"><td><p>if ndarray was displayed how a matrix is displayed in matlab or R, it would be much more convenient</p></td>
</tr>
<tr class="row-even"><td><p>I came from IDL and when I write the same algorithm in IDL it is almost always faster. Part of that is that I understand how to write to take advantage of the parts of IDL that are fast more than I do with NumPy, but even when I use the community-accepted best option, it is usually slower.</p></td>
</tr>
<tr class="row-odd"><td><p>Ways to perform array selection based off criterion for the indices and data value at the index simultaneously.</p></td>
</tr>
<tr class="row-even"><td><p>Develop a portable binary file storage format for use from different languages (C, C++ and Fortran) Borrow ideas from pandas and incorporate in numpy. Pandas is slow compared to numpy but more robust that numpy.</p></td>
</tr>
<tr class="row-odd"><td><p>Use  of GPU for some calculations</p></td>
</tr>
<tr class="row-even"><td><p>Performance on the overall library is really good. It’s just that as my priorities go, it is the most important.</p></td>
</tr>
<tr class="row-odd"><td><p>There are certain tasks that require multiple passes of arrays that simply don’t need to do so, due to numpy’s heavy leaning into masks for certain queries.  Numba of course helps mitigate a lot of these, but that’s a crutch for an obvious design flaw in numpy at it’s core.</p></td>
</tr>
<tr class="row-even"><td><p>There is not much to improve, I put it there as it should always be the highest priority when changing/adding code and features</p></td>
</tr>
<tr class="row-odd"><td><p>Make everything even faster :)</p></td>
</tr>
<tr class="row-even"><td><p>Make it easier to new users to access high performance code by means of jit or something similar.</p></td>
</tr>
<tr class="row-odd"><td><p>The performance of numpy is very important, as important as documentation and reliability. (Original in Spanish: El rendimiento de numpy es muy potente y esa potencia debería de ser igual como en documentación y fiabilidad.)</p></td>
</tr>
<tr class="row-even"><td><p>It is important to measure the execution time looking for opportunities for improvement. (Original in Spanish: Es importante medir el tiempo de ejecución buscando oportunidades de mejora.)</p></td>
</tr>
<tr class="row-odd"><td><p>Improving performance through parallelism. (Original in Spanish: Mejorando el rendimiento por medio de paralelismo.)</p></td>
</tr>
<tr class="row-even"><td><p>More support for parallel programming, CUDA or vectorization of custom functions.</p></td>
</tr>
<tr class="row-odd"><td><p>Parallel computing, improved algorithms</p></td>
</tr>
<tr class="row-even"><td><p>GPU support</p></td>
</tr>
<tr class="row-odd"><td><p>Use code acceleration (GPU, TPU) similar to PyTorch and JAX. (Original in Russian: Использовать различные ускорения кода (GPU, TPU). Например как в PyTorch, JAX.)</p></td>
</tr>
<tr class="row-even"><td><p>Performance is great. It should just stay a priority.</p></td>
</tr>
<tr class="row-odd"><td><p>I have used numpy as the basic library in many large research projects, performance is usually more than adequate. A challenge has been to profile large programs using numpy.</p></td>
</tr>
<tr class="row-even"><td><p>Make it “Faster”!</p></td>
</tr>
<tr class="row-odd"><td><p>Fix Memory usage, restrictions and support on various platforms.</p></td>
</tr>
<tr class="row-even"><td><p>JIT compilation, easy parallel / GPU support</p></td>
</tr>
<tr class="row-odd"><td><p>improve memory-access critical operations</p></td>
</tr>
<tr class="row-even"><td><p>Native distributed, multi-threaded numpy.</p></td>
</tr>
<tr class="row-odd"><td><p>I guess I’m really looking at Numba and Dask for this…</p></td>
</tr>
<tr class="row-even"><td><p>Improved speed, intuitive function design. (Original in Japanese: 速度の向上、直感的に操作できるような関数の設計.)</p></td>
</tr>
<tr class="row-odd"><td><p>The performance is quite good for my uses - I just think it should be maintained</p></td>
</tr>
<tr class="row-even"><td><p>It’s already good but it can be made even better.</p></td>
</tr>
<tr class="row-odd"><td><p>To process and handle large distributed data.</p></td>
</tr>
<tr class="row-even"><td><p>gpu support</p></td>
</tr>
<tr class="row-odd"><td><p>Logic that processes large numbers of calculations as constraints. (Original in Japanese: 大量計算を拘束に処理するロジック.)</p></td>
</tr>
<tr class="row-even"><td><p>the race for better technology is always won by performance and reliability. That’s why I kept them in higher priority.</p></td>
</tr>
<tr class="row-odd"><td><p>Add Numba like JIT support. A specific set of IR can be purposed.</p></td>
</tr>
<tr class="row-even"><td><p>Support to Gpus</p></td>
</tr>
<tr class="row-odd"><td><p>Adding GPU support for accelerated matrix operations</p></td>
</tr>
<tr class="row-even"><td><p>Further improvements in multicore and multithreading. Packages to support hpc usage</p></td>
</tr>
<tr class="row-odd"><td><p>I use NumPy because it is very fast at most tasks.</p></td>
</tr>
<tr class="row-even"><td><p>General speed improvements.</p></td>
</tr>
<tr class="row-odd"><td><p>e.g. Put np.min and np.max in one func</p></td>
</tr>
<tr class="row-even"><td><p>Complex expressions may create temporary arrays behind the scenes. Would be nice if clever coding in the interior of numpy eliminated this.</p></td>
</tr>
<tr class="row-odd"><td><p>It is perfect.</p></td>
</tr>
<tr class="row-even"><td><p>NumPy is outdated in terms of leveraging new hardware instructions and there is a lot of room for improvement.</p></td>
</tr>
<tr class="row-odd"><td><p>Goes together with ‘New Feature’ below: I would like a better way of addressing specific axis in a multidimensional array. Right now I’m tampering with numpy.s_ and direct calls to <strong>getitem</strong>, but I’m not satisfied with that. It is also not usable in numba compiled code and this limits me.</p></td>
</tr>
<tr class="row-even"><td><p>I don’t have many ideas on the topic. I know that there have been already a lot of work on performance, and that more SSE/AVX vectorized implementations are on the way. I imagine things can always be improved somewhat given enough effort of benchmarking, profiling etc.</p></td>
</tr>
<tr class="row-odd"><td><p>Plus easy to optimize the programs. (Original in French: Plus facile optimiser les programmes.)</p></td>
</tr>
<tr class="row-even"><td><p>GPU</p></td>
</tr>
<tr class="row-odd"><td><p>Explicit standardization of core API so other tools can be swapped in as backends for parallel or GPU computing</p></td>
</tr>
<tr class="row-even"><td><p>making use of GPU</p></td>
</tr>
<tr class="row-odd"><td><p>Integrate new features from Intel MKL &amp; BLAS libraries. Optimize performance on new processors with high core counts like AMD’s Threadripper CPUs.</p></td>
</tr>
<tr class="row-even"><td><p>Maybe via Numba.</p></td>
</tr>
<tr class="row-odd"><td><p>Automatically take advantage of multi cores where feasible</p></td>
</tr>
<tr class="row-even"><td><p>Better Sync with DL based frameworks.</p></td>
</tr>
<tr class="row-odd"><td><p>NumPy differential equation solving is slow.</p></td>
</tr>
<tr class="row-even"><td><p>Automatic detection of GPU, support for multithreading</p></td>
</tr>
<tr class="row-odd"><td><p>Heterogenous ops</p></td>
</tr>
<tr class="row-even"><td><p>More documentation on best practices for performance, tips and tricks etc, would make it easier to get the performance that may already be there but currently requires a lot of extra knowledge to obtain.</p></td>
</tr>
<tr class="row-odd"><td><p>Would be great if Numpy had native GPU support, although CuPy essentially makes most applications possible. Would be great to have more options for distributed array computing.</p></td>
</tr>
<tr class="row-even"><td><p>Make use of GPU acceleration  (e.g. allow use of alternative underlying Fortran code with added OpenACC directives, even if only for some common operations)</p></td>
</tr>
<tr class="row-odd"><td><p>Continue to find most computationally efficient methods of executing code.</p></td>
</tr>
<tr class="row-even"><td><p>Community organizing around nep-18</p></td>
</tr>
<tr class="row-odd"><td><p>You might be able to provide a separate option to inherently parallelize certain functions, and/or provide examples on using numpy with other packages that might boost performance.</p></td>
</tr>
<tr class="row-even"><td><p>I don’t think, performance is an issue as of now. It is just, that I think (in general) numpy is in a state where I don’t have much to complain about and performance gains are always nice…</p></td>
</tr>
<tr class="row-odd"><td><p>Reduce memory consumption. (Original in Spanish: Reducir el consumo de memoria.)</p></td>
</tr>
<tr class="row-even"><td><p>Faster code is always helpful</p></td>
</tr>
<tr class="row-odd"><td><p>Parallelization, GPU</p></td>
</tr>
<tr class="row-even"><td><p>Add GPU Support</p></td>
</tr>
<tr class="row-odd"><td><p>NumPy should use numba.</p></td>
</tr>
<tr class="row-even"><td><p>Numpy is really fast, and I don’t have a ton of experience in making it faster. However, I always fall back on numpy for code optimization and I think it is an important place to focus resources.</p></td>
</tr>
<tr class="row-odd"><td><p>expanded randomized linear algebra routines</p></td>
</tr>
<tr class="row-even"><td><p>Automatic execution on GPUs</p></td>
</tr>
<tr class="row-odd"><td><p>I primarily use NumPy to analyze large data sets. Loading those data sets (up to 2GB of csv files) with NumPy is not the most time efficient and eats up a lot of memory.</p></td>
</tr>
<tr class="row-even"><td><p>I have no knowledge on this subject, but believe NumPy’s main usefulness to the scientific community is high performance. This should continue to be a priority.</p></td>
</tr>
<tr class="row-odd"><td><p>Enable gpu support wherever it can be used</p></td>
</tr>
<tr class="row-even"><td><p>One specific problem (which is not purely numpy related) I have is the difficulty to parallelize my Python codes, given the ease in other languages like Julia and C++</p></td>
</tr>
<tr class="row-odd"><td><p>Provide benchmark suite that we can run on platforms at our disposal. Python, owing to its nature (e.g. OpenCV interfaces), is slower compared to same set of operations on other platforms. This handicap makes it a tougher “sell” in mainstream IT departments.</p></td>
</tr>
<tr class="row-even"><td><p>Performance should always be top priority.</p></td>
</tr>
<tr class="row-odd"><td><p>I’m not sure. Just make sure it performs well.</p></td>
</tr>
<tr class="row-even"><td><p>Make sure it works as efficiently as possible.</p></td>
</tr>
<tr class="row-odd"><td><p>Performance is the key value prop for numpy versus pure Python implementations of algorithms.</p></td>
</tr>
<tr class="row-even"><td><p>The performance concerns are mainly around masked arrays, which can be extraordinarily slow.</p></td>
</tr>
<tr class="row-odd"><td><p>Identify areas where current algorithms are highly sub-optimal or where there are advanced algorithms that perform much better.  I think of numpy and scipy together here. For example, numpy median filters are much slower than implementations in other languages such as IDL, and are far slower than optimal algorithms.  The last time I tested, the numpy histogram algorithm was very slow.</p></td>
</tr>
<tr class="row-even"><td><p>The work on vectorization sounds great. There are also some parts of NumPy that could be optimized (like 2D regular binned histograms). As far as I know, the Windows NumPy does not yet include the expression fusing that Unix does, which would be nice. (Most these are just light-weight suggestions, I work further down from NumPy usually, but since so much relies on NumPy, performance gains can affect a huge community)</p></td>
</tr>
<tr class="row-odd"><td><p>1. The core of Numpy written in C should use the new hpy API so that it would be possible for PyPy (and other Python implementations) to accelerate Numpy code.  2. It should be clearly mentioned in the website / documentation that to get high performance, numerical kernels have to be accelerated with tools like Transonic, Numba, Pythran, Cython, … See http://www.legi.grenoble-inp.fr/people/Pierre.Augier/transonic-vision.html</p></td>
</tr>
<tr class="row-even"><td><p>I work in HPC so I am always thinking about performance. Many functions are already fast but more speed is always better. :)</p></td>
</tr>
<tr class="row-odd"><td><p>I don’t have any specific thoughts, but faster is better :)</p></td>
</tr>
<tr class="row-even"><td><p>I’d love to see NumPy as <em>the</em> de facto array computing API, with other packages voluntarily choosing to allow interoperability.</p></td>
</tr>
<tr class="row-odd"><td><p>In general, I’m pretty happy with the current state of NumPy, so general performance gains of numeric routines and aggregations still be a great way to continue to improve NumPy.</p></td>
</tr>
<tr class="row-even"><td><p>It would be really nice if the FFT in NumPy were “best in class.” Typically FFTW performs better than KissFFT (which I think is the basis for NumPy) but there are licensing issues so FFTW itself can’t be used.</p></td>
</tr>
<tr class="row-odd"><td><p>I’m very impressed with the direction of performance - keep on rocking!</p></td>
</tr>
<tr class="row-even"><td><p>Improve interoperability with numba &amp; cupy.</p></td>
</tr>
<tr class="row-odd"><td><p>Very large data set performance</p></td>
</tr>
<tr class="row-even"><td><p>Improve support for intrinsics / SIMD instructions.</p></td>
</tr>
<tr class="row-odd"><td><p>Integration of acceleration by GPU natively. (Original in Spanish: Integración de la aceleración por gpu de forma nativa.)</p></td>
</tr>
<tr class="row-even"><td><p>I found the performance of NumPy fantastic.</p></td>
</tr>
<tr class="row-odd"><td><p>Guides and recommendations or use cases on how to perform the functions in a parallelized way. (Original in Spanish: Guías y recomendaciones o implementaciones para realizar las funciones de forma paralelizada.)</p></td>
</tr>
<tr class="row-even"><td><p>parallelization, SIMD</p></td>
</tr>
<tr class="row-odd"><td><p>There are increasing opportunities for processing large amounts of data. I hope it will be a little faster. (Original in Japanese: 大量データの処理の機会が増えてきています。) 少しでも早くなることを期待します</p></td>
</tr>
<tr class="row-even"><td><p>Create standard benchmarks so they can be evaluated. (Original in Japanese: 標準ベンチマークを作成して、評価できるようにする.)</p></td>
</tr>
<tr class="row-odd"><td><p>Increase processing speed. (Original in Japanese: 処理速度を早くする.)</p></td>
</tr>
<tr class="row-even"><td><p>Speed ​​up of vectorized functions. (Original in Japanese: ベクトル化した関数の速度向上.)</p></td>
</tr>
<tr class="row-odd"><td><p>Jit compiler (numba/torchscript like maybe) to allow “kernel fusion” e.g. make for i in x:   for j in y:     for k in z:        some complex operation per matrix entry fast.</p></td>
</tr>
<tr class="row-even"><td><p>Numpy has great performance; however, the project is “old”. Perhaps a reformulation in the basic code would bring an even better performance. (Original in Portuguese: Numpy tem ótima performance; contudo, o projeto é “antigo”. Talvez uma reformulação no código básico traria uma performance ainda melhor.)</p></td>
</tr>
<tr class="row-odd"><td><p>Parallel computing support, memory saving, or organizing documents to do them. (Original in Japanese: 並列計算の支援、省メモリ、あるいはそれらを行うためのドキュメントの整理.)</p></td>
</tr>
<tr class="row-even"><td><p>Improve the use of partner libraries or reformulate the code, aiming at high performance and preparation for use in quantum computing. (Original in Portuguese: Melhorar o uso de bibliotecas parceiras ou códigos reformulados, visando performance elevada e preparação para uso em computação quântica.)</p></td>
</tr>
<tr class="row-odd"><td><p>Make it faster and more efficient</p></td>
</tr>
<tr class="row-even"><td><p>Doing performance benchmarks with other libs that does the same (maybe even in other languages).</p></td>
</tr>
<tr class="row-odd"><td><p>Improve algorithms implementations with latest research</p></td>
</tr>
<tr class="row-even"><td><p>After getting done from SIMD optimizations, I was thinking about to add direct support for OpenCL &amp; Cuda</p></td>
</tr>
<tr class="row-odd"><td><p>Speed especially with very large arrays</p></td>
</tr>
<tr class="row-even"><td><p>No specific thoughts on how to improve - just that I believe performance is a cornerstone of what makes NumPy so valuable and widely used (in conjunction with the expressive array syntax). IMO it is important for NumPy to maintain high performance to prevent ecosystem fragmentation with the introduction of many new array computation libraries (e.g. PyTorch).</p></td>
</tr>
<tr class="row-odd"><td><p>More widespread usage of multithreading.</p></td>
</tr>
<tr class="row-even"><td><p>It is not really necessary to improve performance and reliability in any specific way. But it is important that you maintain them and keep good performance as it was before.</p></td>
</tr>
<tr class="row-odd"><td><p>Benchmark against Julia.</p></td>
</tr>
<tr class="row-even"><td><p>I think there are ideas about using more vectorisation (with SIMD instructions), which seems interesting. More generally better performance with respect to use of multiple CPUs and memory allocation is always good to take.</p></td>
</tr>
<tr class="row-odd"><td><p>Numpy in already extremely fast. But as more and more data is coming, Numpy has to be faster in many operations</p></td>
</tr>
<tr class="row-even"><td><p>Include ARM compliance and performance testing as a priority (with a view to macOS but also recent super computer usage).</p></td>
</tr>
<tr class="row-odd"><td><p>Increase use of vector instruction (AVX, Neon …) Make numpy more friendly to JIT compiler like numba</p></td>
</tr>
<tr class="row-even"><td><p>faster small arrays</p></td>
</tr>
<tr class="row-odd"><td><p>Allow SIMD access from all platforms, move to more modern glibc beyond manylinux2014</p></td>
</tr>
<tr class="row-even"><td><p>continued work on SIMD acceleration of ufuncs, etc.</p></td>
</tr>
<tr class="row-odd"><td><p>Make it go faster and better. Benchmarked optimizations.</p></td>
</tr>
<tr class="row-even"><td><p>Integrate simd and task-based multithreading (e.g. tbb) throughout</p></td>
</tr>
<tr class="row-odd"><td><p>Keep doing what you are. I don’t think performance is broken as it stands.</p></td>
</tr>
<tr class="row-even"><td><p>I dont have specific targets, just anything that makes it generally faster is helpful in nearly all scenarios</p></td>
</tr>
<tr class="row-odd"><td><p>NumPy is becoming the de facto way of handling data, it should strive to be the best possible tool.</p></td>
</tr>
<tr class="row-even"><td><p>Exposing C APIs for more modules, to be used via Cython</p></td>
</tr>
<tr class="row-odd"><td><p>Multiple low-level backends (eg MKL) with dynamical switching. OpenMP low-level parallelism</p></td>
</tr>
<tr class="row-even"><td><p>Compare the python library random to numpy.random. there are certain scenarios where one is better than the other. For instance, if you want to generate multiple random numbers random.uniform is better; but numpy.random.sample is superior for sampling from a list. It’d be nice to have numpy superior at all times</p></td>
</tr>
<tr class="row-odd"><td><p>Scaling out, compatibility with other libraries or data types</p></td>
</tr>
<tr class="row-even"><td><p>Build a stronger, highly active community. I would like to see seminars and conferences like tensorflow</p></td>
</tr>
<tr class="row-odd"><td><p>Do multi threaded or multi processing calculations wherever possible without or with very few user interaction.</p></td>
</tr>
<tr class="row-even"><td><p>There isn’t a performance shortfall, but continuing to optimize the library and say optimized is always important.</p></td>
</tr>
<tr class="row-odd"><td><p>I am just a user of NumPy (and a big fan!), but what I see with performance is: / 1. People not efficiently using NumPy so that they think NumPy is too slow, just because their code is not optimized. This is why documentation and presenting the most optimized solutions is so important.  / 2. I tried messing around with CuPy and I had some graphic card issues, so my experience is limited, but is it crazy or out of scope to want GPU support in NumPy for some things?</p></td>
</tr>
<tr class="row-even"><td><p>I’m always interested in improved performance. It doesn’t require any work from me but I still benefit from it</p></td>
</tr>
<tr class="row-odd"><td><p>The linear algebra module could see some improvement, (not my area of expertise, so I’m not sure how it could be improved).</p></td>
</tr>
<tr class="row-even"><td><p>I’d love to see support for accelerators and massive parallelism (via XLA as in Jax for example) land in NumPy.</p></td>
</tr>
<tr class="row-odd"><td><p>Speed up computation process. Currently, we need other packages like Numba, NumExpr, or Cupy. It will be interesting to have speed without any additional packages.</p></td>
</tr>
</tbody>
</table>


              </div>
              
              
              <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
              
          </main>
          

      </div>
    </div>
  
    <script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>
  <footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2020, NumPy Survey Team.<br>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.4.0.<br>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>